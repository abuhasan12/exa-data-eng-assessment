"""
The main ETL pipeline file.
Extracts the JSON data, one resource at a time, from the JSON files in the specified file path.
Transforms the JSON data by converting each dictionary-like structure to a row of values.
Loads the rows generated by the transformations to the database tables in batches.
"""


import os
import configparser

from src.database.database_functions.database_connection import create_connection
from src.etl.pipeline_functions import extract, transform, load
from src.etl.resources.schemas import SCHEMAS


def pipeline(file_paths: list, server_config: dict):
    """
    Pipeline function creates a dictionary to hold the lists of all the rows for each resource type to load into the database.
    Calls the extract_resources() function to extract each resource one by one from all the files, loops over them, and transforms them to rows.
    Each returned row by transform_resource() is appended to the dictionary for that resrouce type.
    If any list reaches 1000 items, the 1000 rows are inserted to the table for that resource as a batch.
    Once the iterations are completed, any remaining rows in the dictionary are inserted.

    :param file_paths:
        List of strings of valid file paths of the JSON files to process.
    :param server_config:
        The database server configuration dictionary which includes host, port, user, password, and database information.
    """
    # Logging
    logs_dir = 'logs'
    if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
    num_logs = len(os.listdir(logs_dir))

    # Initialise dictionary to hold the rows of data.
    data_rows = {
        'CarePlan': [],
        'Claim': [],
        'Condition': [],
        'DiagnosticReport': [],
        'DocumentReference': [],
        'Encounter': [],
        'ExplanationOfBenefit': [],
        'MedicationRequest': [],
        'Patient': [],
        'Procedure': []
    }
    
    # Connect to database.
    conn = create_connection(server_config=server_config)

    # Iterate over resources from the JSON data extracted from the files.
    for resource in extract.extract_resources(file_paths):

        resource_type = resource['resource']['resourceType']

        # Only transform resource types in dictionary.
        if resource['resource']['resourceType'] in data_rows:
            transformed_resource = transform.transform_resource(resource)

            # Append the transformed row to the dictionary.
            data_rows[resource_type].append(transformed_resource)
        
            # If row amounts reach 1000, insert to database as batch.
            if len(data_rows[resource_type]) == 1000:
                table_name = SCHEMAS[resource_type]['table_meta']['table_name']
                num_cols = len(SCHEMAS[resource_type]['json_schema'])

                load.upload_resources(conn, table_name, num_cols, data_rows[resource_type])

                # Empty list of rows for next batch.
                data_rows[resource_type] = []

    # Loop through the data if any left and insert the data as batches to table
    for resource_type, rows in data_rows.items():
        if rows:
            table_name = SCHEMAS[resource_type]['table_meta']['table_name']
            num_cols = len(SCHEMAS[resource_type]['json_schema'])

            load.upload_resources(conn, table_name, num_cols, rows)
            
    conn.close()

    if len(os.listdir(logs_dir)) > num_logs:
        print("WARNING: There were some errors when inserting to database. Check log file.")


def main():
    """
    Running this file will insert all the processed data extracted and processed from each file to the database.
    """
    # Specify the directory holding the data.
    data_dir = "data/"
    file_paths = [os.path.join(data_dir, file_name) for file_name in os.listdir(data_dir)]

    config = configparser.ConfigParser()

    # Read postgreSQL configuration from config.ini file.
    try:
        print("Reading credentials...")
        config.read('config.ini')

        # Set up config dictionary.
        server_config = {
            'HOST': config.get('Server', 'HOST'),
            'PORT': config.get('Server', 'PORT'),
            'USER': config.get('Server', 'USER'),
            'PASSWORD': config.get('Server', 'PASSWORD'),
            'DATABASE': config.get('Server', 'DATABASE')
        }
        print("Credential loaded.")
    except configparser.Error as e:
        raise Exception(f"Could not read config file due to error: {e}")
    
    # Run pipeline.
    pipeline(file_paths=file_paths, server_config=server_config)

if __name__ == '__main__':
    main()